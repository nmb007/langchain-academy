{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# LangChain Academy\n",
    "\n",
    "Welcome to LangChain Academy! \n",
    "\n",
    "## Context\n",
    "\n",
    "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible. \n",
    "\n",
    "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state. \n",
    "\n",
    "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
    "\n",
    "## Course Structure\n",
    "\n",
    "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independently of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) because it is both popular and performant. As noted, please ensure that you have an `OPENAI_API_KEY`.\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6316930-40df-469d-a97f-c71f2f5cf591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/naveed/projects/learning/langgraph/lc-academy-env/bin/python3.13\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a15227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "#_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "[Here](https://python.langchain.com/v0.2/docs/how_to/#chat-models) is a useful how-to for all the things that you can do with chat models, but we'll show a few highlights below. If you've run `pip install -r requirements.txt` as noted in the README, then you've installed the `langchain-openai` package. With this, we can instantiate our `ChatOpenAI` model object. If you are signing up for the API for the first time, you should receive [free credits](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517) that can be applied to any of the models. You can see pricing for various models [here](https://openai.com/api/pricing/). The notebooks will default to `gpt-4o` because it's a good balance of quality, price, and speed [see more here](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini), but you can also opt for the lower priced `gpt-3.5` series models. \n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) that we can set with chat models. Two of the most common are:\n",
    "\n",
    "* `model`: the name of the model\n",
    "* `temperature`: the sampling temperature\n",
    "\n",
    "`Temperature` controls the randomness or creativity of the model's output where low temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses. High temperature (close to 1) is good for creative tasks or generating varied responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: stream back chunks of the response\n",
    "* `invoke`: call the chain on an input\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1280e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 + 6 = 9.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 16, 'total_tokens': 25, 'completion_time': 0.020624974, 'prompt_time': 0.002571251, 'queue_time': 0.001945296, 'total_time': 0.023196225}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5062cfdd-3bdf-46a7-b5eb-596b56ad9dae-0', usage_metadata={'input_tokens': 16, 'output_tokens': 9, 'total_tokens': 25})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"whats 3+6?\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27c6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 12, 'total_tokens': 22, 'completion_time': 0.022906116, 'prompt_time': 0.002497528, 'queue_time': 0.002291258, 'total_time': 0.025403644}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--65e31bd6-efd6-4566-b2e3-0f9623b589cb-0', usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc2f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 12, 'total_tokens': 22, 'completion_time': 0.02292115, 'prompt_time': 0.002451543, 'queue_time': 0.002140099, 'total_time': 0.025372693}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--73299738-e656-40fa-b4d9-2cbcb3fbd9d9-0', usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {},
   "source": [
    "## Search Tools\n",
    "\n",
    "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091dff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d69da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"Who won 1992 cricket world cup ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bafd7d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '1992 Cricket World Cup - Wikipedia',\n",
       "  'url': 'https://en.wikipedia.org/wiki/1992_Cricket_World_Cup',\n",
       "  'content': \"The \\\\\\\\1992 Cricket World Cup\\\\\\\\ (known as the \\\\\\\\Benson & Hedges World Cup 1992\\\\\\\\ for sponsorship reasons) was the fifth Cricket World Cup, the premier One Day Internationalcricket tournament for men's national teams, organised by the International Cricket Council (ICC). It was held in Australia and New Zealand from 22 February to 25 March 1992, and finished with Pakistan beating England by 22 runs in the final to become the World Cup champions for the first time. The tournament is remembered for [...] From Wikipedia, the free encyclopedia\\nBenson & Hedges World Cup 1992Image 1: Tournament logo\\nDates 22 February – 25 March 1992\\nAdministrator(s)International Cricket Council\\nCricket formatOne Day International\\nTournament format(s)Round-robin and Knockout\\nHost(s)\\\\ Australia\\n\\\\ New Zealand\\nChampionsImage 2Pakistan (1st title)\\nRunners-upImage 3England\\nParticipants 9\\nMatches 39\\nPlayer of the seriesImage 4: New ZealandMartin Crowe\\nMost runsImage 5: New ZealandMartin Crowe (456) [...] Co-hosts New Zealand proved the surprise package of the tournament, winning their first seven consecutive games to finish on top of the table after the round-robin. The other hosts, Australia, one of the pre-tournament favourites lost their first two matches. They recovered somewhat to win four of the remaining six, but narrowly missed out on the semi-finals. The West Indies also finished with a 4–4 record, but were just behind Australia on run-rate. South Africa made a triumphant return to\",\n",
       "  'score': 0.95334905},\n",
       " {'title': '1992 Cricket World Cup final',\n",
       "  'url': 'https://en.wikipedia.org/wiki/1992_Cricket_World_Cup_final',\n",
       "  'content': 'Cricket final\\n\\nThe final of the 1992 ICC Cricket World Cup was played at the Melbourne Cricket Ground, Melbourne on 25 March 1992. The match was won by Pakistan, under the captaincy of Imran Khan, as they defeated England by 22 runs to lift their first ever World Cup trophy. This was the second Cricket World Cup final to be played outside England and the first in Australia. 87,182 spectators turned out to see the final.\\n\\n## Details\\n\\n[edit]\\n\\n### Match officials\\n\\n[edit] [...] | The Melbourne Cricket Ground | |\\n| Event | 1992 ICC Cricket World Cup |\\n| | Pakistan | England |  --- | |  |  | | 249/6 | 227 | | 50 overs | 49.2 overs | | |\\n| Pakistan won by 22 runs | |\\n| Date | 25 March 1992 |\\n| Venue | Melbourne Cricket Ground, Melbourne |\\n| Player of the match \"Player of the Match awards (cricket)\") | Wasim Akram (Pak) |\\n| Umpires \"Umpire (cricket)\") | Brian Aldridge \"Brian Aldridge (cricketer)\") (NZ) and Steve Bucknor (WI) |\\n| Attendance | 87,182 |\\n| ← 1987  1996 → | | [...] |  |\\n\\n| 25 March 1992  scorecard |\\n\\n|  |  |  |\\n --- \\n| Pakistan 249/6 (50 overs) | v | England 227 (49.2 overs) |\\n| Imran Khan 72 (110)  Derek Pringle 3/22 (10 overs) |  | Neil Fairbrother 62 (70)  Mushtaq Ahmed \"Mushtaq Ahmed (cricketer)\") 3/41 (10 overs) |\\n\\n|  |\\n\\n| Pakistan won by 22 runs  Melbourne Cricket Ground, Melbourne  Attendance : 87,182  Umpires: Brian Aldridge \"Brian Aldridge (cricketer)\") (NZ) and Steve Bucknor (WI)  Player of the match: Wasim Akram (Pak) |',\n",
       "  'score': 0.92142147},\n",
       " {'title': 'England vs Pakistan, Final, Benson Hedges World Cup, 1992',\n",
       "  'url': 'https://www.cricbuzz.com/live-cricket-scorecard/6802/eng-vs-pak-final-benson-hedges-world-cup-1992',\n",
       "  'content': 'Series:  Benson Hedges World Cup, 1992 Venue: Melbourne Cricket Ground, Melbourne Date & Time: Mar 25, 11:00 AM LOCAL\\n\\nPakistan won by 22 runs\\n\\nPakistan Innings 249-6 (50 Ov)\\n\\nBatter\\n\\nR\\n\\nB\\n\\n4s\\n\\n6s\\n\\nSR\\n\\nAamer Sohail\\n\\nc Stewart b Pringle\\n\\n4\\n\\n19\\n\\n0\\n\\n0\\n\\n21.05\\n\\nRamiz Raja\\n\\nlbw b Pringle\\n\\n8\\n\\n26\\n\\n1\\n\\n0\\n\\n30.77\\n\\nImran Khan\\n\\nc Illingworth b Botham\\n\\n72\\n\\n110\\n\\n5\\n\\n1\\n\\n65.45\\n\\nJaved Miandad\\n\\nc Botham b Illingworth\\n\\n58\\n\\n98\\n\\n4\\n\\n0\\n\\n59.18\\n\\nInzamam-ul-Haq\\n\\nb Pringle\\n\\n42\\n\\n35\\n\\n4\\n\\n0\\n\\n120.00\\n\\nWasim Akram [...] 6-1 (I Botham, 0), 21-2 (A Stewart, 0), 59-3 (G Hick, 0), 69-4 (G Gooch, 0), 141-5 (A Lamb, 0), 141-6 (C Lewis, 0), 180-7 (N Fairbrother, 0), 183-8 (D Reeve, 0), 208-9 (P DeFreitas, 47.1), 227-10 (R Illingworth, 49.2)\\n\\nBowler\\n\\nO\\n\\nM\\n\\nR\\n\\nW\\n\\nNB\\n\\nWD\\n\\nECO\\n\\nWasim Akram\\n\\n10\\n\\n0\\n\\n49\\n\\n3\\n\\n4\\n\\n6\\n\\n4.90\\n\\nAaqib Javed\\n\\n10\\n\\n2\\n\\n27\\n\\n2\\n\\n1\\n\\n3\\n\\n2.70\\n\\nMushtaq Ahmed\\n\\n10\\n\\n1\\n\\n41\\n\\n3\\n\\n0\\n\\n1\\n\\n4.10\\n\\nIjaz Ahmed\\n\\n3\\n\\n0\\n\\n13\\n\\n0\\n\\n0\\n\\n2\\n\\n4.33\\n\\nImran Khan\\n\\n6.2\\n\\n0\\n\\n43\\n\\n1\\n\\n1\\n\\n0\\n\\n6.94\\n\\nAamer Sohail\\n\\n10\\n\\n0\\n\\n49\\n\\n0\\n\\n0\\n\\n1\\n\\n4.90\\n\\nMatch Info',\n",
       "  'score': 0.86413884}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fbbbf-74f6-40be-9961-2968e1f0f284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
